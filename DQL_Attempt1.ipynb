{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOmsu4Dg2xqfmHD5T1xMJSh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejasgadgil/RNN-for-KI/blob/main/DQL_Attempt1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qb-wqfFg2bRr",
        "outputId": "7f1f81b6-9745-4b54-b341-dc7d9b58e9c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.49)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.4.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (5.3.0)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.3.6)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2024.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.8)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Collecting gymnasium<1.1.0,>=0.29.1 (from stable-baselines3)\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.25.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.0)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<1.1.0,>=0.29.1->stable-baselines3)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Downloading stable_baselines3-2.4.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium, stable-baselines3\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0 stable-baselines3-2.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install yfinance pandas numpy matplotlib gym stable-baselines3 tensorflow torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# Fetch historical stock data\n",
        "def fetch_stock_data(ticker, start_date, end_date):\n",
        "    data = yf.download(ticker, start=start_date, end=end_date)\n",
        "    data.reset_index(inplace=True)  # Resets index and keeps 'Date' as a column\n",
        "    return data\n",
        "\n",
        "def normalize_data(data):\n",
        "    # Normalize all columns except 'Date'\n",
        "    numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns\n",
        "    data[numeric_columns] = (data[numeric_columns] - data[numeric_columns].mean()) / data[numeric_columns].std()\n",
        "    return data\n",
        "\n",
        "# Calculate Exponential Moving Average (EMA)\n",
        "def calculate_ema(data, window=10):\n",
        "    # Calculate Exponential Moving Average for 'Close' column\n",
        "    data['EMA'] = data['Close'].ewm(span=window, adjust=False).mean()\n",
        "    return data\n",
        "\n",
        "# Example: Fetch data for AAPL\n",
        "stock_data = fetch_stock_data('AAPL', '2020-01-01', '2024-12-06')\n",
        "stock_data = calculate_ema(stock_data, window=10)  # 10-day EMA\n",
        "stock_data = normalize_data(stock_data)\n",
        "\n",
        "print(stock_data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnS-9vj12gf7",
        "outputId": "6e93aa0d-8ad9-4238-d545-927c168ba7e2"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Price                       Date Adj Close     Close      High       Low  \\\n",
            "Ticker                                AAPL      AAPL      AAPL      AAPL   \n",
            "0      2020-01-02 00:00:00+00:00 -1.917658 -1.932197 -1.962489 -1.931091   \n",
            "1      2020-01-03 00:00:00+00:00 -1.935014 -1.950352 -1.962613 -1.922906   \n",
            "2      2020-01-06 00:00:00+00:00 -1.920927 -1.935617 -1.966454 -1.946337   \n",
            "3      2020-01-07 00:00:00+00:00 -1.929308 -1.944383 -1.960630 -1.916782   \n",
            "4      2020-01-08 00:00:00+00:00 -1.900778 -1.914540 -1.938695 -1.918782   \n",
            "\n",
            "Price       Open    Volume       EMA  \n",
            "Ticker      AAPL      AAPL            \n",
            "0      -1.955601  0.831701 -1.930449  \n",
            "1      -1.949939  1.035119 -1.933772  \n",
            "2      -1.970845  0.511010 -1.933794  \n",
            "3      -1.933202  0.332492 -1.935416  \n",
            "4      -1.949877  0.767890 -1.931281  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating custom gym environment\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from gym import spaces\n",
        "\n",
        "class StockTradingEnv(gym.Env):\n",
        "    def __init__(self, data, render_mode='human'):\n",
        "        super(StockTradingEnv, self).__init__()\n",
        "        self.data = data\n",
        "        self.current_step = 0\n",
        "        self.max_steps = len(data) - 1\n",
        "        self.holding_penalty = 1  # Example value\n",
        "        self.max_hold_duration = 20  # Example value, in steps\n",
        "        self.holding_duration = 0  # Initialize holding duration\n",
        "        self.profit_reward = 10  # Example value\n",
        "        self.loss_penalty = 2  # Example value\n",
        "        self.buy_reward = 1  # Example value\n",
        "\n",
        "\n",
        "\n",
        "        # Action space: Buy, Sell, Hold\n",
        "        self.action_space = spaces.Discrete(3)\n",
        "\n",
        "        num_features = len(data.columns) - 1  # Exclude 'Date'\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf,\n",
        "            high=np.inf,\n",
        "            shape=(num_features + 3,),  # Add 2 for balance and shares, EMA\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        # Initial portfolio\n",
        "        self.balance = 10_00_000  # Starting cash\n",
        "        self.shares = 0       # Number of shares owned\n",
        "        self.total_value = self.balance\n",
        "\n",
        "        self.position = None  # No position initially\n",
        "        self.buy_price = 0  # Track buy price\n",
        "\n",
        "\n",
        "        # Set render mode\n",
        "        self.render_mode = render_mode\n",
        "\n",
        "    def reset(self):\n",
        "        self.current_step = 0\n",
        "        self.balance = 10_00_000\n",
        "        self.shares = 0\n",
        "        self.total_value = self.balance\n",
        "        self.history = []  # Initialize log history\n",
        "        self.position = None  # Reset position\n",
        "        self.holding_duration = 0  # Reset holding duration\n",
        "        return self._next_observation()\n",
        "\n",
        "\n",
        "    def _next_observation(self):\n",
        "        # Ensure that we're correctly handling the data and balance/shares/EMA\n",
        "        stock_data = self.data.iloc[self.current_step].drop(['Date']).values.astype(np.float32)  # Stock data (1D array)\n",
        "\n",
        "        # Balance, shares, and EMA should be reshaped to 1D arrays\n",
        "        balance = np.array([self.balance], dtype=np.float32).reshape(-1)  # Balance (1D array)\n",
        "        shares = np.array([self.shares], dtype=np.float32).reshape(-1)    # Shares (1D array)\n",
        "        ema = np.array([self.data.iloc[self.current_step]['EMA']], dtype=np.float32).reshape(-1)  # EMA (1D array)\n",
        "\n",
        "        # Ensure all arrays have the same shape before hstack\n",
        "        obs = np.hstack([stock_data, balance, shares, ema])\n",
        "        return obs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode == 'human':\n",
        "            # Display the environment to the human (e.g., graphical window)\n",
        "            pass\n",
        "        elif self.render_mode == 'rgb_array':\n",
        "            # Return an array of the environment's current state\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported render mode: {self.render_mode}\")\n",
        "\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0\n",
        "        done = False\n",
        "        info = {}\n",
        "\n",
        "        # Get current price\n",
        "        self.current_price = float(self.data.iloc[self.current_step]['Close'])\n",
        "\n",
        "\n",
        "        # Action: Buy\n",
        "        if action == 0 and self.position != 'Bought' and float(self.balance) >= float(self.current_price):\n",
        "            self.shares += int(self.balance // self.current_price)\n",
        "            self.balance -= self.shares * self.current_price\n",
        "            self.position = 'Bought'\n",
        "            self.buy_price = self.current_price  # Record the buy price\n",
        "            self.holding_duration = 0  # Reset holding duration\n",
        "            reward += self.buy_reward  # Reward for buying\n",
        "\n",
        "        # Action: Sell\n",
        "        elif action == 1 and self.shares > 0 :\n",
        "            profit_or_loss = (float(self.current_price) - float(self.buy_price)) * self.shares\n",
        "            self.balance += self.shares * self.current_price\n",
        "            self.shares = 0\n",
        "            self.position = \"None\"  # Reset position after selling\n",
        "            self.holding_duration = 0  # Reset holding duration\n",
        "            # profit_or_loss = profit_or_loss.item() if isinstance(profit_or_loss, pd.Series) else profit_or_loss\n",
        "\n",
        "            if profit_or_loss > 0:  # Loss\n",
        "                reward += self.profit_reward * profit_or_loss\n",
        "            else:  # Profit\n",
        "                reward -= self.loss_penalty * abs(profit_or_loss)\n",
        "\n",
        "            # self.buy_price = 0  # Clear buy price\n",
        "\n",
        "        # Action: Hold\n",
        "        elif action == 2 :\n",
        "            self.holding_duration += 1\n",
        "            reward -= 0.01  # Apply a small penalty for holding\n",
        "            if self.holding_duration > self.max_hold_duration:\n",
        "                reward -= self.holding_penalty  # Apply holding penalty\n",
        "\n",
        "        # Check stopping conditions\n",
        "        if self.balance <= 0:  # No money left\n",
        "            done = True\n",
        "            reward -= 30  # Large penalty for running out of money\n",
        "        elif self.balance >= 1.3 * 10_00_000:  # Balance has doubled\n",
        "            done = True\n",
        "            reward += 60  # Large reward for achieving the goal\n",
        "\n",
        "        # Move to the next state\n",
        "        self.current_step += 1\n",
        "        if self.current_step >= self.max_steps:\n",
        "            done = True\n",
        "\n",
        "        # Define the next state\n",
        "        next_state = self._next_observation()  # Update or calculate the next state based on the action\n",
        "\n",
        "        # Log additional info if needed\n",
        "        info = {\n",
        "            'balance': self.balance,\n",
        "            'shares': self.shares,\n",
        "            'total_value': self.balance + (self.shares * self.current_price)\n",
        "        }\n",
        "\n",
        "        return next_state, reward, done, info\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KSc1d4ks2nS1"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'shimmy>=2.0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHwjMJWU3XAK",
        "outputId": "bd1a884f-6a1c-4e08-9b02-7a5ad4ee8193"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shimmy>=2.0 in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from shimmy>=2.0) (1.26.4)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.10/dist-packages (from shimmy>=2.0) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=1.0.0a1->shimmy>=2.0) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import itertools\n",
        "\n",
        "# # Define the EarlyStoppingCallback class\n",
        "# class EarlyStoppingCallback(BaseCallback):\n",
        "#     def __init__(self, patience=10, min_improvement=0.01, verbose=0):\n",
        "#         super().__init__(verbose)\n",
        "#         self.patience = patience\n",
        "#         self.min_improvement = min_improvement\n",
        "#         self.best_reward = -np.inf\n",
        "#         self.patience_counter = 0\n",
        "\n",
        "#     def _on_step(self) -> bool:\n",
        "#         # Get the current episode reward from the environment\n",
        "#         rewards = self.locals['rewards']\n",
        "#         if len(rewards) > 0:\n",
        "#             avg_reward = np.mean(rewards)\n",
        "#             print(f\"Average reward: {avg_reward}\")\n",
        "\n",
        "#             # Check if the reward has improved\n",
        "#             if avg_reward > self.best_reward + self.min_improvement:\n",
        "#                 self.best_reward = avg_reward\n",
        "#                 self.patience_counter = 0\n",
        "#             else:\n",
        "#                 self.patience_counter += 1\n",
        "\n",
        "#             # If the reward hasn't improved for `patience` episodes, stop training\n",
        "#             if self.patience_counter >= self.patience:\n",
        "#                 print(\"Early stopping triggered.\")\n",
        "#                 return False  # Stop training\n",
        "#         return True  # Continue training\n",
        "\n",
        "# # Hyperparameter space\n",
        "# params = {\n",
        "#     'learning_rate': [0.0001, 0.001, 0.01],\n",
        "#     'gamma': [0.9, 0.95, 0.99],\n",
        "#     'batch_size': [32, 64, 128],\n",
        "#     'exploration_fraction': [0.8, 0.9],\n",
        "#     'exploration_final_eps': [0.2, 0.05, 0.01],\n",
        "#     'target_update_interval': [1000, 5000, 10000]\n",
        "# }\n",
        "\n",
        "# # Wrap your custom environment\n",
        "# env = StockTradingEnv(stock_data, render_mode='human')\n",
        "# env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# # Function to train and evaluate the model with early stopping\n",
        "# def train_and_evaluate(learning_rate, gamma, batch_size, exploration_fraction, exploration_final_eps, target_update_interval):\n",
        "#     print(f\"Training with lr={learning_rate}, gamma={gamma}, batch_size={batch_size}, exploration_fraction={exploration_fraction}, exploration_final_eps={exploration_final_eps}, target_update_interval={target_update_interval}\")\n",
        "\n",
        "#     model = DQN(\n",
        "#         \"MlpPolicy\",\n",
        "#         env,\n",
        "#         learning_rate=learning_rate,\n",
        "#         gamma=gamma,\n",
        "#         batch_size=batch_size,\n",
        "#         exploration_fraction=exploration_fraction,\n",
        "#         exploration_final_eps=exploration_final_eps,\n",
        "#         exploration_initial_eps=1.0,  # Start with full exploration\n",
        "#         target_update_interval=target_update_interval,\n",
        "#         verbose=1,\n",
        "#     )\n",
        "\n",
        "#     # Create the early stopping callback\n",
        "#     early_stopping_callback = EarlyStoppingCallback(patience=10, min_improvement=0.01, verbose=1)\n",
        "\n",
        "#     # Train the model with the callback\n",
        "#     model.learn(total_timesteps=1_00_000, callback=early_stopping_callback)\n",
        "\n",
        "#     model.save(f\"stock_dqn_model_lr_{learning_rate}_gamma_{gamma}_batch_{batch_size}_exploration_{exploration_fraction}_final_eps_{exploration_final_eps}_target_{target_update_interval}\")\n",
        "#     print(\"Model saved\")\n",
        "\n",
        "# # Grid search for hyperparameter combinations\n",
        "# param_combinations = list(itertools.product(\n",
        "#     params['learning_rate'],\n",
        "#     params['gamma'],\n",
        "#     params['batch_size'],\n",
        "#     params['exploration_fraction'],\n",
        "#     params['exploration_final_eps'],\n",
        "#     params['target_update_interval']\n",
        "# ))\n",
        "\n",
        "# # Train and evaluate for each combination\n",
        "# for combination in param_combinations:\n",
        "#     lr, gamma, batch_size, exploration_fraction, exploration_final_eps, target_update_interval = combination\n",
        "#     train_and_evaluate(lr, gamma, batch_size, exploration_fraction, exploration_final_eps, target_update_interval)\n",
        "\n",
        "\n",
        "# EarlyStoppingCallback for DQN\n",
        "class EarlyStoppingCallback(BaseCallback):\n",
        "    def __init__(self, patience=10, min_improvement=0.01, verbose=0):\n",
        "        super().__init__(verbose)\n",
        "        self.patience = patience\n",
        "        self.min_improvement = min_improvement\n",
        "        self.best_reward = -np.inf\n",
        "        self.patience_counter = 0\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        rewards = self.locals['rewards']\n",
        "        if len(rewards) > 0:\n",
        "            avg_reward = np.mean(rewards)\n",
        "            print(f\"Average reward: {avg_reward}\")\n",
        "\n",
        "            if avg_reward > self.best_reward + self.min_improvement:\n",
        "                self.best_reward = avg_reward\n",
        "                self.patience_counter = 0\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "\n",
        "            if self.patience_counter >= self.patience:\n",
        "                print(\"Early stopping triggered.\")\n",
        "                return False  # Stop training\n",
        "        return True  # Continue training\n",
        "\n",
        "# Hyperparameter space for grid search\n",
        "params = {\n",
        "    'learning_rate': [0.0001, 0.001, 0.01],\n",
        "    'gamma': [0.9, 0.95, 0.99],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    'exploration_fraction': [0.8, 0.9],\n",
        "    'exploration_final_eps': [0.2, 0.05, 0.01],\n",
        "    'target_update_interval': [1000, 5000, 10000]\n",
        "}\n",
        "\n",
        "# Wrap your custom environment\n",
        "env = StockTradingEnv(stock_data, render_mode='human')\n",
        "env = make_vec_env(lambda: env, n_envs=1)\n",
        "\n",
        "# Grid search and training for different hyperparameters\n",
        "param_combinations = list(itertools.product(\n",
        "    params['learning_rate'],\n",
        "    params['gamma'],\n",
        "    params['batch_size'],\n",
        "    params['exploration_fraction'],\n",
        "    params['exploration_final_eps'],\n",
        "    params['target_update_interval']\n",
        "))\n",
        "\n",
        "for combination in param_combinations:\n",
        "    lr, gamma, batch_size, exploration_fraction, exploration_final_eps, target_update_interval = combination\n",
        "    print(f\"Training with lr={lr}, gamma={gamma}, batch_size={batch_size}, exploration_fraction={exploration_fraction}, exploration_final_eps={exploration_final_eps}, target_update_interval={target_update_interval}\")\n",
        "\n",
        "    model = DQN(\n",
        "        \"MlpPolicy\",\n",
        "        env,\n",
        "        learning_rate=lr,\n",
        "        gamma=gamma,\n",
        "        batch_size=batch_size,\n",
        "        exploration_fraction=exploration_fraction,\n",
        "        exploration_final_eps=exploration_final_eps,\n",
        "        exploration_initial_eps=1.0,  # Start with full exploration\n",
        "        target_update_interval=target_update_interval,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    model.learn(total_timesteps=100_000, callback=EarlyStoppingCallback(patience=5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ggBhmyv2_or",
        "outputId": "440e918c-d3bd-48e8-d48a-df61aa05d3a9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-56-b639490aaff0>:60: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
            "  stock_data = self.data.iloc[self.current_step].drop(['Date']).values.astype(np.float32)  # Stock data (1D array)\n",
            "<ipython-input-56-b639490aaff0>:94: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  self.current_price = float(self.data.iloc[self.current_step]['Close'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.5      |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 204      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6        |\n",
            "----------------------------------\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.75     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 233      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 7        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.75     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 214      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 7        |\n",
            "----------------------------------\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.5      |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 178      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6        |\n",
            "----------------------------------\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 145      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.5      |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 157      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.5      |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 219      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6        |\n",
            "----------------------------------\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.5      |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 162      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.75     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 156      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 7        |\n",
            "----------------------------------\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 125      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 113      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 208      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.5      |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 198      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6        |\n",
            "----------------------------------\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.75     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 130      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 7        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.75     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 254      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 7        |\n",
            "----------------------------------\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.0001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 250      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 241      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 240      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.5      |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 265      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6        |\n",
            "----------------------------------\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.5      |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 283      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 201      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.5      |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 249      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6        |\n",
            "----------------------------------\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.5      |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 189      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6        |\n",
            "----------------------------------\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1        |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 206      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 4        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 267      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.001, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.5      |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 236      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6        |\n",
            "----------------------------------\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.5      |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 254      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6        |\n",
            "----------------------------------\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.5      |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 247      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 6        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.9, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 151      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 196      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 165      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 206      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.95, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=32, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 179      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 216      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=64, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1        |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 160      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 4        |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 186      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 10       |\n",
            "----------------------------------\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.8, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -29.0\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 1.25     |\n",
            "|    ep_rew_mean      | -29      |\n",
            "|    exploration_rate | 1        |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 240      |\n",
            "|    time_elapsed     | 0        |\n",
            "|    total_timesteps  | 5        |\n",
            "----------------------------------\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.2, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.05, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=1000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=5000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: 0.0\n",
            "Average reward: -0.009999999776482582\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n",
            "Training with lr=0.01, gamma=0.99, batch_size=128, exploration_fraction=0.9, exploration_final_eps=0.01, target_update_interval=10000\n",
            "Using cpu device\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: 0.0\n",
            "Average reward: 0.0\n",
            "Average reward: -29.0\n",
            "Average reward: -0.009999999776482582\n",
            "Early stopping triggered.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset environment\n",
        "obs = env.reset()\n",
        "done = False\n",
        "\n",
        "# Simulate environment and collect actions\n",
        "actions = []  # To store actions and rewards\n",
        "while not done:\n",
        "    # Use the VecEnv observation directly\n",
        "    action, _states = model.predict(obs, deterministic=False)  # Set deterministic to False for exploration\n",
        "    action = action.item()  # Convert action to a scalar if it's a numpy array or list\n",
        "\n",
        "    # Wrap the action in a list for VecEnv compatibility\n",
        "    action = [action]  # Wrap the action in a list\n",
        "\n",
        "    # Take a step in the environment\n",
        "    obs, reward, done, info = env.step(action)\n",
        "\n",
        "    print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
        "\n",
        "    actions.append({\"action\": action, \"reward\": reward})\n",
        "\n"
      ],
      "metadata": {
        "id": "uKWYS5rc6kCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3ad441-b831-4678-f909-14f10b4991c1"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action: [1], Reward: [0.], Done: [False]\n",
            "Action: [2], Reward: [-0.01], Done: [False]\n",
            "Action: [0], Reward: [-29.], Done: [ True]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-56-b639490aaff0>:60: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
            "  stock_data = self.data.iloc[self.current_step].drop(['Date']).values.astype(np.float32)  # Stock data (1D array)\n",
            "<ipython-input-56-b639490aaff0>:94: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
            "  self.current_price = float(self.data.iloc[self.current_step]['Close'])\n"
          ]
        }
      ]
    }
  ]
}